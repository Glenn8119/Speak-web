# Threads response

Use LangGraph.

## Thread 1

1. I plan to use Streamable HTTP to communicate with AI, the /chat endpoint is responsible for AI response and corrections, and I plan to use LangGraph parallel execution for both, it's okay to emit either first as long as either of them is generated by AI.

2. Hide the correction first unless the user clicks it, in terms of UI, it acts like an accordian component below user messages.

3. Show all the corrections inline. (The corrections should be message-level, so it should be only one correction.)

## Thread 2

Actually we need to store the conversation in LangGraph's thread, so that the conversation exists even after the user refreshes the page.

## Thread 3

Two calls, use parallel execution described in my response in thread 1.

The corrections should be message-level.

## Thread 4

1. The summary model contains two parts, the first part is to show all corrections (this part can acheive without AI), the second part is to give user tips and suggestions on how to practice these problems.

2. It's not exportable but we will use Notion MCP to sycn to users Notion in the future, we can just leave a TODO comment in the backend for now.

## Architecture Sketch

We have two endpoints

1. /chat - responsible for AI response and corrections
2. /summary - collect corrections and give practice tips.

## ðŸ’­ What You're Curious About

1. For now, it's okey to use text only, we'll implement voice stream in the future.
2. The corrections only show after users type.
3. We focus on correcting users' grammar errors, we don't require strict formal Englishâ€”natural, everyday grammar is perfectly fine.
4. It's more like a quick practiceï¼Œuser can click "Summary" button to generate summary anytime they want
5. We use Anthropic which is supported in LangGraph
